import numpy as np # Is it version 2.1 the one you are running?
import matplotlib.pyplot as plt
import torch # Is it version 2.4 the one you are running?
import torch.nn as nn
import torch.optim as optim

'''
Code Bonus question
'''
def create_bonus_dataset(x_range, sample_size, sigma, seed=42):
    torch.manual_seed(seed)
    x_min, x_max = x_range
    x = torch.rand(sample_size)
    x = x * (x_max - x_min) + x_min

    y_hat = 2 * torch.log(x+1)+3

    # Compute y (Add Gaussian noise) with mean=0 and std_dev=sigma
    y = y_hat + torch.normal(torch.zeros(sample_size), sigma*torch.ones(sample_size))

    return x, y
    

def visualize_bonus_data(x_true, y_true, x_gen, y_gen, label):
    plt.plot(x_true, y_true, label="True function", color="b")
    plt.scatter(x_gen, y_gen, label=label+' data', alpha=0.5)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('True function vs. data generated for '+label+' of f(x) = 2log(x + 1) + 3\nwith x interval = [-0.05,' + str(a) + ']')
    plt.legend()
    plt.show()


if __name__ == '__main__':
    sigma = 0.5
    train_sample_size = 500
    train_seed = 0
    val_sample_size = 500
    val_seed = 1
    z_range = (-2,2)

    a_values = [0.01, 10]

    for a in a_values:
        x = np.linspace(-0.05, a)
        f = 2 * np.log(x+1)+3

        x_range = (-0.05, a)
        sigma = 0.5

        x_train, y_train = create_bonus_dataset(x_range, train_sample_size, sigma, train_seed)
        x_val, y_val = create_bonus_dataset(x_range, val_sample_size, sigma, val_seed)

        visualize_bonus_data(x, f, x_train, y_train, 'Training')
        visualize_bonus_data(x, f, x_val, y_val, 'Validation')

        model = nn.Linear(1, 1)
        loss_fn = nn.MSELoss()
        learning_rate = 0.05
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)

        x_train = x_train.reshape(-1, 1)
        x_val = x_val.reshape(-1, 1)
        y_train = y_train.reshape(-1, 1)
        y_val = y_val.reshape(-1, 1)

        n_steps=200

        train_loss_vals = []
        val_loss_vals = []

        for step in range(n_steps):
            model.train()
            optimizer.zero_grad()
            # Compute the output of the model
            y_hat = model(x_train)
            # Compute the loss
            loss = loss_fn(y_hat, y_train)

            # Compute the gradient
            loss.backward()
            # Update the parameters
            optimizer.step()
            with torch.no_grad():
                # Compute the output of the model
                y_hat_val = model(x_val)
                # Compute the loss
                loss_val = loss_fn(y_hat_val, y_val)

                val_loss_vals.append(loss_val.item())
                train_loss_vals.append(loss.item())

        print("Step:", step, "- Loss eval:", loss_val.item(), "- Loss train:", loss.item())

        plt.plot(range(step + 1), train_loss_vals)
        plt.plot(range(step + 1), val_loss_vals)
        plt.title("Train Loss vs Evaluation Loss\nwith x in [-0.05,"+str(a)+"]")
        plt.legend(["Training loss", "Validation loss"])
        plt.xlabel("Steps")
        plt.ylabel("Loss value")
        plt.show()
